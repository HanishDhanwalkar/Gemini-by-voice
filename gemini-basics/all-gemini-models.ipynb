{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()  # Load environment variables from the `.env` file \n",
    "\n",
    "api_key = os.getenv(\"API_KEY\") # replace with your API_KEY\n",
    "genai.configure(api_key= api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(name='models/chat-bison-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='PaLM 2 Chat (Legacy)',\n",
      "      description='A legacy text-only model optimized for chat conversations',\n",
      "      input_token_limit=4096,\n",
      "      output_token_limit=1024,\n",
      "      supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
      "      temperature=0.25,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/text-bison-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='PaLM 2 (Legacy)',\n",
      "      description='A legacy model that understands text and generates text as an output',\n",
      "      input_token_limit=8196,\n",
      "      output_token_limit=1024,\n",
      "      supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
      "      temperature=0.7,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/embedding-gecko-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Embedding Gecko',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=1024,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedText', 'countTextTokens'],\n",
      "      temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-1.0-pro',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro',\n",
      "      description='The best model for scaling across a wide range of tasks',\n",
      "      input_token_limit=30720,\n",
      "      output_token_limit=2048,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.9,\n",
      "      top_p=1.0,\n",
      "      top_k=1)\n",
      "Model(name='models/gemini-1.0-pro-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
      "      description=('The best model for scaling across a wide range of tasks. This is a stable '\n",
      "                   'model that supports tuning.'),\n",
      "      input_token_limit=30720,\n",
      "      output_token_limit=2048,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
      "      temperature=0.9,\n",
      "      top_p=1.0,\n",
      "      top_k=1)\n",
      "Model(name='models/gemini-1.0-pro-latest',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro Latest',\n",
      "      description=('The best model for scaling across a wide range of tasks. This is the latest '\n",
      "                   'model.'),\n",
      "      input_token_limit=30720,\n",
      "      output_token_limit=2048,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.9,\n",
      "      top_p=1.0,\n",
      "      top_k=1)\n",
      "Model(name='models/gemini-1.0-pro-vision-latest',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro Vision',\n",
      "      description='The best image understanding model to handle a broad range of applications',\n",
      "      input_token_limit=12288,\n",
      "      output_token_limit=4096,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.4,\n",
      "      top_p=1.0,\n",
      "      top_k=32)\n",
      "Model(name='models/gemini-pro',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro',\n",
      "      description='The best model for scaling across a wide range of tasks',\n",
      "      input_token_limit=30720,\n",
      "      output_token_limit=2048,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.9,\n",
      "      top_p=1.0,\n",
      "      top_k=1)\n",
      "Model(name='models/gemini-pro-vision',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro Vision',\n",
      "      description='The best image understanding model to handle a broad range of applications',\n",
      "      input_token_limit=12288,\n",
      "      output_token_limit=4096,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.4,\n",
      "      top_p=1.0,\n",
      "      top_k=32)\n",
      "Model(name='models/embedding-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Embedding 001',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=2048,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedContent'],\n",
      "      temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/aqa',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Model that performs Attributed Question Answering.',\n",
      "      description=('Model trained to return answers to questions that are grounded in provided '\n",
      "                   'sources, along with estimating answerable probability.'),\n",
      "      input_token_limit=7168,\n",
      "      output_token_limit=1024,\n",
      "      supported_generation_methods=['generateAnswer'],\n",
      "      temperature=0.2,\n",
      "      top_p=1.0,\n",
      "      top_k=40)\n"
     ]
    }
   ],
   "source": [
    "models_available = []\n",
    "\n",
    "for model in genai.list_models():\n",
    "    pprint.pprint(model)\n",
    "    name = re.findall(r\"display_name='(.*?)'\", str(model))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['models/chat-bison-001', 'PaLM 2 Chat (Legacy)'], ['models/text-bison-001', 'PaLM 2 (Legacy)'], ['models/embedding-gecko-001', 'Embedding Gecko'], ['models/gemini-1.0-pro', 'Gemini 1.0 Pro'], ['models/gemini-1.0-pro-001', 'Gemini 1.0 Pro 001 (Tuning)'], ['models/gemini-1.0-pro-latest', 'Gemini 1.0 Pro Latest'], ['models/gemini-1.0-pro-vision-latest', 'Gemini 1.0 Pro Vision'], ['models/gemini-pro', 'Gemini 1.0 Pro'], ['models/gemini-pro-vision', 'Gemini 1.0 Pro Vision'], ['models/embedding-001', 'Embedding 001'], ['models/aqa', 'Model that performs Attributed Question Answering.'], ['models/chat-bison-001', 'PaLM 2 Chat (Legacy)'], ['models/text-bison-001', 'PaLM 2 (Legacy)'], ['models/embedding-gecko-001', 'Embedding Gecko'], ['models/gemini-1.0-pro', 'Gemini 1.0 Pro'], ['models/gemini-1.0-pro-001', 'Gemini 1.0 Pro 001 (Tuning)'], ['models/gemini-1.0-pro-latest', 'Gemini 1.0 Pro Latest'], ['models/gemini-1.0-pro-vision-latest', 'Gemini 1.0 Pro Vision'], ['models/gemini-pro', 'Gemini 1.0 Pro'], ['models/gemini-pro-vision', 'Gemini 1.0 Pro Vision'], ['models/embedding-001', 'Embedding 001'], ['models/aqa', 'Model that performs Attributed Question Answering.']]\n",
      "                                         Display_name  \\\n",
      "0                                PaLM 2 Chat (Legacy)   \n",
      "1                                     PaLM 2 (Legacy)   \n",
      "2                                     Embedding Gecko   \n",
      "3                                      Gemini 1.0 Pro   \n",
      "4                         Gemini 1.0 Pro 001 (Tuning)   \n",
      "5                               Gemini 1.0 Pro Latest   \n",
      "6                               Gemini 1.0 Pro Vision   \n",
      "7                                      Gemini 1.0 Pro   \n",
      "8                               Gemini 1.0 Pro Vision   \n",
      "9                                       Embedding 001   \n",
      "10  Model that performs Attributed Question Answer...   \n",
      "\n",
      "                             model_name  \\\n",
      "0                 models/chat-bison-001   \n",
      "1                 models/text-bison-001   \n",
      "2            models/embedding-gecko-001   \n",
      "3                 models/gemini-1.0-pro   \n",
      "4             models/gemini-1.0-pro-001   \n",
      "5          models/gemini-1.0-pro-latest   \n",
      "6   models/gemini-1.0-pro-vision-latest   \n",
      "7                     models/gemini-pro   \n",
      "8              models/gemini-pro-vision   \n",
      "9                  models/embedding-001   \n",
      "10                           models/aqa   \n",
      "\n",
      "                                          Description version  \n",
      "0   A legacy text-only model optimized for chat co...     001  \n",
      "1   A legacy model that understands text and gener...     001  \n",
      "2      Obtain a distributed representation of a text.     001  \n",
      "3   The best model for scaling across a wide range...     001  \n",
      "4                                                None     001  \n",
      "5                                                None     001  \n",
      "6   The best image understanding model to handle a...     001  \n",
      "7   The best model for scaling across a wide range...     001  \n",
      "8   The best image understanding model to handle a...     001  \n",
      "9      Obtain a distributed representation of a text.     001  \n",
      "10                                               None     001  \n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns = ['Display_name', 'model_name', 'Description', 'version'])\n",
    "\n",
    "\n",
    "for model in genai.list_models():\n",
    "\n",
    "    Display_name = re.findall(r\"display_name='(.*?)'\", str(model))\n",
    "    name = re.findall(r\"name='(.*?)'\", str(model))\n",
    "    version=  re.findall(r\"version='(.*?)'\", str(model))\n",
    "    Description =  re.findall(r\"description='(.*?)'\", str(model))\n",
    "\n",
    "    if len(Description) > 0:\n",
    "        Description = Description[0]\n",
    "    else:\n",
    "        Description = None\n",
    "\n",
    "    df = df._append({'Display_name': Display_name[0], 'model_name' : name[0], 'Description': Description, 'version': version[0]}, ignore_index=True)\n",
    "    models_available.append(name)\n",
    "\n",
    "print(models_available)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Gemini-models.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
